{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments:\n",
    "1. Install PyTorch manually https://pytorch.org/get-started/locally/\n",
    "2. Run `pip install -e .[finetune]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase\n",
    "from transformers.models.qwen2 import Qwen2ForCausalLM\n",
    "from transformers.tokenization_utils_base import BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model and tokenizer\n",
    "model_id = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "model: Qwen2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the output head only\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are Git Commit Message Pro, a specialist in crafting precise, professional Git commit messages from .diff files. Your role is to analyze these files, interpret the changes, and generate a clear, direct commit message.\n",
    "\n",
    "Guidelines:\n",
    "1. Be specific about the type of change (e.g., \"Rename variable X to Y\", \"Extract method Z from class W\").\n",
    "2. Prefer to write it on why and how instead of what changed.\n",
    "3. Interpret the changes; do not transcribe the diff.\n",
    "4. If you cannot read the entire file, attempt to generate a message based on the available information.\n",
    "5. Be concise and summarize the most important changes. Keep your response in 1 sentence.\"\"\"\n",
    "\n",
    "\n",
    "def make_input(diff: str, message: str | None):\n",
    "    \"\"\"Create a conversation-like input for modelling\"\"\"\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": instruction + \"\\n\\nInputs:\\n\" + diff},\n",
    "    ]\n",
    "    if message is not None:\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": message})\n",
    "    return conversation\n",
    "\n",
    "\n",
    "def extra_pad_to_ignore(labels: list[int], pad_id: int, ignore_idx: int = -100):\n",
    "    \"\"\"Convert padding to ignore index after the first pad token\"\"\"\n",
    "    if pad_id in labels:\n",
    "        first_pad_idx = labels.index(pad_id)\n",
    "        for i, l in enumerate(labels):\n",
    "            if i > first_pad_idx and l == pad_id:\n",
    "                labels[i] = ignore_idx\n",
    "    return labels\n",
    "\n",
    "\n",
    "def non_completion_to_ignore(\n",
    "    labels: list[int],\n",
    "    begin_phrase: list[int],\n",
    "    end_phrase: list[int],\n",
    "    ignore_idx: int = -100,\n",
    "):\n",
    "    \"\"\"Convert all labels outside each begine_phrase, end_phrase pair to ignore, excluding those phrases\n",
    "\n",
    "    More: https://towardsdatascience.com/to-mask-or-not-to-mask-the-effect-of-prompt-tokens-on-instruction-tuning-016f85fd67f4/\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a result list initialized with the ignore index\n",
    "    result = [ignore_idx] * len(labels)\n",
    "\n",
    "    # Flags to indicate whether we are inside a valid phrase\n",
    "    inside_phrase = False\n",
    "\n",
    "    # Lengths of the phrases for easier reference\n",
    "    begin_length = len(begin_phrase)\n",
    "    end_length = len(end_phrase)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(labels):\n",
    "        # Check for begin_phrase\n",
    "        if labels[i : i + begin_length] == begin_phrase:\n",
    "            inside_phrase = True\n",
    "            # Mark the begin phrase\n",
    "            result[i : i + begin_length] = begin_phrase\n",
    "            i += begin_length\n",
    "            continue\n",
    "\n",
    "        # Check for end_phrase\n",
    "        if labels[i : i + end_length] == end_phrase and inside_phrase:\n",
    "            inside_phrase = False\n",
    "            # Mark the end phrase\n",
    "            result[i : i + end_length] = end_phrase\n",
    "            i += end_length\n",
    "            continue\n",
    "\n",
    "        # If we're inside a valid phrase, keep the label\n",
    "        if inside_phrase:\n",
    "            result[i] = labels[i]\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the tokenizer and tokens processing\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"World\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(conversation, tokenize=False))\n",
    "print(tokenizer.apply_chat_template(conversation))\n",
    "print(tokenizer.encode(\"<|im_start|>assistant\\n\"))\n",
    "print(\n",
    "    non_completion_to_ignore(\n",
    "        tokenizer.apply_chat_template(conversation),\n",
    "        tokenizer.encode(\"<|im_start|>assistant\\n\"),\n",
    "        tokenizer.encode(\"<|im_end|>\\n\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Any\n",
    "\n",
    "import datasets as hf_data\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    data: dict[str, Any], tokenizer: PreTrainedTokenizerBase, seq_length: int = 4096\n",
    "):\n",
    "    \"\"\"\n",
    "    From the given data, do the followings:\n",
    "        1. form a conversation from the instruction, diff and message\n",
    "        2. tokenize the input\n",
    "        3. pad, mask and truncate to the given seq_length\n",
    "        4. generate target labels by shifting input tokens by 1\n",
    "    \"\"\"\n",
    "    conversation = make_input(data[\"diff\"], data.get(\"message\", None))\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        padding=\"max_length\",\n",
    "        return_dict=True,\n",
    "        max_length=seq_length + 1,  # for shifting the targets\n",
    "        truncation=True,\n",
    "    )\n",
    "    assert isinstance(tokens, BatchEncoding)\n",
    "    labels: list[int] = tokens[\"input_ids\"][1:]\n",
    "    labels = extra_pad_to_ignore(labels, tokenizer.pad_token)\n",
    "    labels = non_completion_to_ignore(\n",
    "        labels,\n",
    "        tokenizer.encode(\"<|im_start|>assistant\\n\"),\n",
    "        tokenizer.encode(\"<|im_end|>\"),\n",
    "    )\n",
    "\n",
    "    for k, v in tokens.items():\n",
    "        tokens[k] = v[:-1]  # crop after shifting\n",
    "    return {**tokens, \"labels\": labels, \"conversation\": conversation}\n",
    "\n",
    "\n",
    "dataset_dict = hf_data.load_dataset(\"Maxscha/commitbench\")\n",
    "assert isinstance(dataset_dict, hf_data.DatasetDict)\n",
    "for split, dataset in dataset_dict.items():\n",
    "    dataset: hf_data.Dataset\n",
    "    size = int(len(dataset))\n",
    "    dataset_dict[split] = dataset.shuffle(42).select(range(size))\n",
    "\n",
    "dataset_dict = dataset_dict.filter(lambda data: data[\"diff_languages\"] == \"py\")\n",
    "dataset_dict = dataset_dict.map(\n",
    "    partial(preprocess, tokenizer=tokenizer, seq_length=1024)\n",
    ")\n",
    "# only keep not truncated\n",
    "dataset_dict = dataset_dict.filter(lambda data: data[\"attention_mask\"][-1] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_dict)\n",
    "sample = dataset_dict[\"train\"][1]\n",
    "print(\"Len\", len(sample[\"input_ids\"]), sample[\"input_ids\"][:10])\n",
    "print(len(sample[\"labels\"]))\n",
    "conver = sample[\"conversation\"]\n",
    "for message in conver:\n",
    "    print(f\"{message['role']}: {message['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/AlexandrosChrtn/llama-fine-tune-guide\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results-3B\",\n",
    "    # use_cpu=True,\n",
    "    # eval_strategy=\"steps\",  # to evaluate during training\n",
    "    # eval_steps=1000,\n",
    "    logging_steps=200,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    per_device_train_batch_size=2,  # Adjust based on your hardware\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,  # How many times to loop through the dataset\n",
    "    # fp16=False,  # Must be False for MacBooks\n",
    "    # report_to=\"none\",  # Here we can use something like tensorboard to see the training metrics\n",
    "    # log_level=\"info\",\n",
    "    learning_rate=2e-5,  # Would avoid larger values here\n",
    "    max_grad_norm=2,  # Clipping the gradients is always a good idea\n",
    ")\n",
    "\n",
    "\n",
    "class GenerationCallback(TrainerCallback):\n",
    "    \"\"\"Save sample outputs to txt file\"\"\"\n",
    "    def __init__(self, folder: Path) -> None:\n",
    "        super().__init__()\n",
    "        self.folder = folder\n",
    "        folder.mkdir(exist_ok=True, parents=True)\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\", model=model, device_map=\"auto\", tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    def on_log(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        random_samples = dataset_dict[\"test\"].shuffle().select(range(5))\n",
    "        for sample in random_samples:\n",
    "            sample: dict\n",
    "            conversation = sample[\"conversation\"][:-1]\n",
    "            outputs = self.generator(\n",
    "                conversation,\n",
    "                return_full_text=False,\n",
    "                num_return_sequences=4,\n",
    "                max_new_tokens=128,\n",
    "            )\n",
    "            target = sample[\"message\"]\n",
    "\n",
    "            file = self.folder / f\"generation-s{state.global_step}.txt\"\n",
    "            with file.open(\"a+\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"----- Input -----\\n\")\n",
    "                for con in conversation:\n",
    "                    f.write(con[\"role\"].upper() + \": \" + con[\"content\"] + \"\\n\")\n",
    "                f.write(\"----- Target -----\\n\")\n",
    "                f.write(target + \"\\n\")\n",
    "                f.write(\"----- Generation -----\\n\")\n",
    "                for i, gen in enumerate(outputs):\n",
    "                    f.write(f\"{i}. {gen['generated_text']}\" + \"\\n\")\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[GenerationCallback(Path(\"results-3B/samples\"))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from transformers import pipeline\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.TextGenerationPipeline\n",
    "# https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "# https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/text_generation#transformers.GenerationMixin.generate\n",
    "generator = pipeline(\n",
    "    \"text-generation\", model=model, device_map=\"auto\", tokenizer=tokenizer\n",
    ")\n",
    "sample = dataset_dict[\"test\"][0]\n",
    "conversation = sample[\"conversation\"][:-1]\n",
    "print(conversation[0][\"content\"])\n",
    "outputs = generator(\n",
    "    conversation,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=4,\n",
    "    max_new_tokens=128,\n",
    "    # do_sample=True,\n",
    ")\n",
    "pprint(outputs)\n",
    "print(\"Target:\", sample[\"conversation\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "if \"HF_TOKEN\" not in os.environ:\n",
    "    login()\n",
    "model.push_to_hub(\"git-commit-3B\")\n",
    "tokenizer.push_to_hub(\"git-commit-3B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
